{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_exploration.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3yjxFE3Z0mz0vhTLChVdS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qvkBB5_B2D2","executionInfo":{"status":"ok","timestamp":1658013572216,"user_tz":420,"elapsed":3337,"user":{"displayName":"Leonid Shpaner","userId":"10226027130831758558"}},"outputId":"3c2d8fcf-9ee8-48ce-a285-2d117205f764"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["%cd /content/drive/Shared drives/Capstone - Best Group/navigating_crime/Code Library/data_exploration"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEj2o_juCNtM","executionInfo":{"status":"ok","timestamp":1658013572217,"user_tz":420,"elapsed":19,"user":{"displayName":"Leonid Shpaner","userId":"10226027130831758558"}},"outputId":"b3a69846-0ba4-4cf0-e691-407ebdb6853b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shared drives/Capstone - Best Group/navigating_crime/Code Library/data_exploration\n"]}]},{"cell_type":"code","source":["####################################\n","## import the requisite libraries ##\n","####################################\n","\n","import os\n","import csv\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tabulate import tabulate \n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Vw_K0zqfCn79","executionInfo":{"status":"ok","timestamp":1658013573071,"user_tz":420,"elapsed":863,"user":{"displayName":"Leonid Shpaner","userId":"10226027130831758558"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# check current working directory\n","eda_path = os.getcwd()\n","eda_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"FfOMPZq3CcsS","executionInfo":{"status":"ok","timestamp":1658013573072,"user_tz":420,"elapsed":7,"user":{"displayName":"Leonid Shpaner","userId":"10226027130831758558"}},"outputId":"02003593-b9f9-4be8-ec6e-b61ffc807650"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/Shared drives/Capstone - Best Group/navigating_crime/Code Library/data_exploration'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["data_path = '/content/drive/Shared drives/Capstone - Best Group/navigating_crime/Dataset/' +\\\n","            'Capstone_LACity_Walking_Streets_With_Crimes.csv'"],"metadata":{"id":"yOBMpGWLCwoe","executionInfo":{"status":"ok","timestamp":1658013573073,"user_tz":420,"elapsed":6,"user":{"displayName":"Leonid Shpaner","userId":"10226027130831758558"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# read in the csv file to a dataframe using pandas\n","df = pd.read_csv(data_path)\n","df.head()"],"metadata":{"id":"zl-NjLOaCrtN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Report"],"metadata":{"id":"zI030TEQJDTi"}},{"cell_type":"code","source":["def build_report(df):\n","\n","  # create an empty log container for appending to output dataframe \n","  log_txt = []\n","\n","  # create starting empty header for new line\n","  header1 = ' '\n","  print(header1)\n","  # append header to log_txt container defined above\n","  log_txt.append(header1)\n","\n","  header2 = 'File Contents'\n","  print(header2)\n","  log_txt.append(header2)\n","  header3 = ' '\n","  print(header3)\n","  log_txt.append(header3)\n","  header4 = 'No. of Rows in File: ' + str(f\"{df.shape[0]:,}\")\n","  print(header4)\n","  log_txt.append(header4)\n","  header5 = 'No. of Columns in File: ' + str(f\"{df.shape[1]:,}\")\n","  print(header5)\n","  log_txt.append(header5)\n","  header6 = ' '\n","  print(header6)\n","  log_txt.append(header6)\n","\n","  header7 = 'ID Column Information'\n","  print(header7)\n","  log_txt.append(header7)\n","\n","  # filter out any columns contain the 'ID' string\n","  id_col = df.filter(like='ID').columns\n","  # if there are any columns that contain the '_id' string in df\n","  # print the number of unique columns and get a distinct count\n","  # otherwise, report that these Ids do not exist.\n","\n","  if df[id_col].columns.any():\n","    df_print = df[id_col].nunique().apply(lambda x : \"{:,}\".format(x))\n","    df_print = pd.DataFrame(df_print)\n","    df_print.reset_index(inplace=True)\n","    df_print = df_print.rename(columns={0: 'Distinct Count',\n","                                            'index':'ID Columns'})\n","    # encapsulate this distinct count within a table\n","    df_tab = tabulate(df_print, headers='keys', tablefmt='psql')\n","    print(df_tab)\n","    log_txt.append(df_tab)\n","  else:\n","    df_notab = 'Street IDs DO NOT exist.'\n","    print(df_notab)\n","    log_txt.append(df_notab)\n","  \n","  header8 = ' '\n","  print(header8)\n","  log_txt.append(header8)\n","\n","  header9 = 'Zip Code Column Information'\n","  print(header9)\n","  log_txt.append(header9)\n","\n","  # filter out any columns contain the 'Zip' string\n","  zip_col = df.filter(like='Zip').columns\n","  # if there are any columns that contain the 'Zip' string in df\n","  # print the number of unique columns and get a distinct count\n","  # otherwise, report that these Ids do not exist.\n","\n","  if df[zip_col].columns.any():\n","    df_print = df[zip_col].nunique().apply(lambda x : \"{:,}\".format(x))\n","    df_print = pd.DataFrame(df_print)\n","    df_print.reset_index(inplace=True)\n","    df_print = df_print.rename(columns={0: 'Distinct Count',\n","                                            'index':'ID Columns'})\n","    # encapsulate this distinct count within a table\n","    df_tab = tabulate(df_print, headers='keys', tablefmt='psql')\n","    print(df_tab)\n","    log_txt.append(df_tab)\n","  else:\n","    df_notab = 'Street IDs DO NOT exist.'\n","    print(df_notab)\n","    log_txt.append(df_notab)\n","\n","  header10 = ' '\n","  print(header10)\n","  log_txt.append(header10)\n","\n","  header11 = 'Date Column Information'\n","  print(header11)\n","  log_txt.append(header11)\n","\n","  # filter out any columns contain the 'Zip' string\n","  date_col = df.filter(like='Date').columns\n","  # if there are any columns that contain the 'Zip' string in df\n","  # print the number of unique columns and get a distinct count\n","  # otherwise, report that these Ids do not exist.\n","\n","  if df[date_col].columns.any():\n","    df_print = df[date_col].nunique().apply(lambda x : \"{:,}\".format(x))\n","    df_print = pd.DataFrame(df_print)\n","    df_print.reset_index(inplace=True)\n","    df_print = df_print.rename(columns={0: 'Distinct Count',\n","                                            'index':'ID Columns'})\n","    # encapsulate this distinct count within a table\n","    df_tab = tabulate(df_print, headers='keys', tablefmt='psql')\n","    print(df_tab)\n","    log_txt.append(df_tab)\n","  else:\n","    df_notab = 'Street IDs DO NOT exist.'\n","    print(df_notab)\n","    log_txt.append(df_notab)\n","\n","  header12 = ' '\n","  print(header12)\n","  log_txt.append(header12)\n","\n","  header13 = 'Column Data Types and Their Respective Null Counts'\n","  print(header13)\n","  log_txt.append(header13)\n","\n","  # Features' Data Types and Their Respective Null Counts\n","  data_types = df.dtypes\n","\n","  # create a new dataframe to inspect data types\n","  data_types = pd.DataFrame(data_types)\n","\n","  # sum the number of nulls per column in df\n","  data_types['Null_Values'] = df.isnull().sum()\n","\n","  # reset index w/ inplace = True for more efficient memory usage\n","  data_types.reset_index(inplace=True)\n","\n","  # percentage of null values is produced and cast to new variable\n","  data_types['perc_null'] = round(data_types['Null_Values'] / len(df)*100,0)\n","  \n","\n","  # columns are renamed for a cleaner appearance\n","  data_types = data_types.rename(columns={0:'Data Type',\n","                                            'index': 'Column/Variable',\n","                                            'Null_Values': '# of Nulls',\n","                                            'perc_null': 'Percent Null'})\n","  \n","  # sort null values in descending order\n","  data_types = data_types.sort_values(by=['# of Nulls'], ascending=False)\n"," \n","\n","  # output data types (show the output for it)\n","  data_types = tabulate(data_types, headers='keys', tablefmt='psql')\n","  print(data_types)\n","  log_txt.append(data_types)\n","\n","  report = pd.DataFrame({'LA City Walking Streets With Crimes Data Report'\n","                         :log_txt})\n","  return report"],"metadata":{"id":"Snw0gl0cEAyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pass the build_report function to a new variable named report\n","report = build_report(df)\n","\n","## DEFINING WHAT GETS SENT OUT\n","\n","# save report to .txt file\n","report.to_csv(eda_path+'/report.txt', \n","              index=False, sep=\"\\t\",\n","              quoting=csv.QUOTE_NONE,  quotechar='', escapechar='\\t')\n","\n","# # save report to .rtf file\n","report.to_csv(eda_path+'/report.rtf', \n","              index=False, sep=\"\\t\",\n","              quoting=csv.QUOTE_NONE, quotechar='', escapechar='\\t')"],"metadata":{"id":"YhbZ1azGEDYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Status_1']"],"metadata":{"id":"UmUfiRNQKDN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create histograms for development data to inspect distributions\n","df.hist(figsize=(30,16), grid=False, density=True)\n","plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, \n","                    top=1, wspace=1.5, hspace=0.5)\n","plt.show()"],"metadata":{"id":"F2Jb1ptRJNxY"},"execution_count":null,"outputs":[]}]}